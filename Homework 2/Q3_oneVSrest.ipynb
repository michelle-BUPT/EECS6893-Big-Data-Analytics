{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create/import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[1.0,22.08,11.46,...|    0|\n",
      "|[0.0,22.67,7.0,2....|    0|\n",
      "|[0.0,29.58,1.75,1...|    0|\n",
      "|[0.0,21.67,11.5,1...|    1|\n",
      "|[1.0,20.17,8.17,2...|    1|\n",
      "|[0.0,15.83,0.585,...|    1|\n",
      "|[1.0,17.42,6.5,2....|    0|\n",
      "|[0.0,58.67,4.46,2...|    1|\n",
      "|[1.0,27.83,1.0,1....|    0|\n",
      "|[0.0,55.75,7.08,2...|    0|\n",
      "|[1.0,33.5,1.75,2....|    1|\n",
      "|[1.0,41.42,5.0,2....|    1|\n",
      "|[1.0,20.67,1.25,1...|    0|\n",
      "|[1.0,34.92,5.0,2....|    1|\n",
      "|[1.0,58.58,2.71,2...|    0|\n",
      "|[1.0,48.08,6.04,2...|    1|\n",
      "|[1.0,29.58,4.5,2....|    1|\n",
      "|[0.0,18.92,9.0,2....|    1|\n",
      "|[1.0,20.0,1.25,1....|    0|\n",
      "|[0.0,22.42,5.665,...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import OneVsRest\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "#lines = spark.read.text('/Users/littleostrichsnewmacbook/Desktop/bigdata-hw2/pima-indians-diabetes.data.txt').rdd\n",
    "#parts = lines.map(lambda row: row.value.split(\",\"))\n",
    "#rdd = parts.map(lambda p: Row(features = \n",
    "#                              Vectors.dense([p[0], p[1], p[2], p[3], p[4], p[5], p[6], p[7]]), \n",
    "#                              label = int(p[8])))\n",
    "lines = spark.read.text('/Users/littleostrichsnewmacbook/Desktop/australian.dat.txt').rdd\n",
    "parts = lines.map(lambda row: row.value.split(\" \"))\n",
    "rdd = parts.map(lambda p: Row(features = \n",
    "                              Vectors.dense([p[0], p[1], p[2], p[3], p[4], p[5], p[6], p[7],\n",
    "                                             p[8], p[9], p[10], p[11], p[12], p[13]]), \n",
    "                              label = int(p[14])))\n",
    "sentenceData = spark.createDataFrame(rdd)\n",
    "sentenceData.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = sentenceData.randomSplit([0.7, 0.3], 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|[0.0,15.75,0.375,...|    0|       0.0|\n",
      "|[0.0,15.83,0.585,...|    1|       1.0|\n",
      "|[0.0,15.83,7.625,...|    0|       0.0|\n",
      "|[0.0,17.92,0.54,2...|    0|       0.0|\n",
      "|[0.0,19.17,0.585,...|    0|       1.0|\n",
      "|[0.0,19.17,5.415,...|    0|       0.0|\n",
      "|[0.0,19.58,0.665,...|    0|       0.0|\n",
      "|[0.0,20.08,0.125,...|    1|       0.0|\n",
      "|[0.0,20.42,7.5,2....|    1|       1.0|\n",
      "|[0.0,20.67,1.835,...|    1|       1.0|\n",
      "|[0.0,20.75,10.25,...|    1|       1.0|\n",
      "|[0.0,20.75,10.335...|    1|       1.0|\n",
      "|[0.0,20.83,3.0,2....|    1|       1.0|\n",
      "|[0.0,21.67,11.5,1...|    1|       1.0|\n",
      "|[0.0,22.5,11.0,1....|    0|       1.0|\n",
      "|[0.0,22.67,0.75,2...|    0|       0.0|\n",
      "|[0.0,23.25,5.875,...|    1|       1.0|\n",
      "|[0.0,23.58,0.585,...|    0|       0.0|\n",
      "|[0.0,23.58,0.83,2...|    0|       0.0|\n",
      "|[0.0,23.75,0.71,2...|    0|       0.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test set accuracy = 0.82153548651\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression().setMaxIter(10).setTol(1E-6)\n",
    "ovr = OneVsRest().setClassifier(classifier)\n",
    "ovrModel = ovr.fit(trainingData)\n",
    "predictions = ovrModel.transform(testData)\n",
    "predictions.show()\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
